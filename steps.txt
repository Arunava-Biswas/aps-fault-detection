Step1: 
- Whether git is install or not? To check just type "git" in the terminal.
- Here we are creating the project with python 3.8.10 version.
- The default mongoDB connection is: "mongodb://localhost:27017", it can be found in the 'main.py' file.





Step2:
- Create dataset inside the mongoDB database.
- To download the dataset from 'git' we use the command 'wget' in terminal.
- To download the data write in the following format and hit enter:
    wget url
- As here it is:
abc@4816ef265cb4:~/workspace$ wget https://raw.githubusercontent.com/avnyadav/sensor-fault-detection/main/aps_failure_training_set1.csv
- After doing this the dataset gets downloaded.
- Now we need to upload the data to mongoDB. To do this we will create the 'data_dump.py' file.





Step3:
- Now we will read the data from the mongoDB and do the Machine Learning project.
- But before we need to add this codes in the github using git commands.
- So go to github and create a new repo. While creating repo (aps-fault-detection) just select Add.gitignore option as 'python'.
- 1st we need to see is there any repositore already attach to this code or not? to check type the following command
    git remote -v

abc@4816ef265cb4:~/workspace$ git remote -v
origin  https://github.com/iNeuron-Pvt-Ltd/neurolab-mongodb-python (fetch)
origin  https://github.com/iNeuron-Pvt-Ltd/neurolab-mongodb-python (push)

- Now to remove the origin so we can create our own repo as origin type:
    git remote remove origin
- Again to check just type:
    git remote -v

abc@4816ef265cb4:~/workspace$ git remote remove origin
abc@4816ef265cb4:~/workspace$ git remote -v

- Now we need to add our repo as origin type:
    git remote add origin <give your url>

abc@4816ef265cb4:~/workspace$ git remote add origin https://github.com/Arunava-Biswas/aps-fault-detection.git
abc@4816ef265cb4:~/workspace$ git remote -v
origin  https://github.com/Arunava-Biswas/aps-fault-detection.git (fetch)
origin  https://github.com/Arunava-Biswas/aps-fault-detection.git (push)

- Now we need to push the changes to make the changes applicable. Here do the authentication steps to allow the repo to joined with this project. type:
    git push origin main

- Here we will get some error message, as there are changes ('.gitignore' file) in the github repo which are not shown here on this project so to make them one first we have to fetch (pull the changes from the github to the local system) the repo here then create the commit and then do the push. This is the 'merge conflict'.

- So either we need to delete the commits in the github repo or we need to delete the commits in our local system as the problem is that the commits are not matching. Here we are deleting the commits in the local repo.
- Go to hidden folder '.git'

abc@4816ef265cb4:~/workspace$ ls -a
.  ..  aps_failure_training_set1.csv  data_dump.py  .git  main.py  README.md  requirements.txt  steps.txt  .vscode

- Now go to inside the '.git' folder

abc@4816ef265cb4:~/workspace$ cd .git/
abc@4816ef265cb4:~/workspace/.git$ ls
branches  config  description  FETCH_HEAD  HEAD  hooks  index  info  logs  objects  ORIG_HEAD  packed-refs  refs

- We need to use the following command:
    git reset --soft ID
- The ID is the id of the commit where we want the pointer HEAD to move.
- With soft the codes remained.

abc@4816ef265cb4:~/workspace$ git reset --soft 6afd

- Now we can create a new commit and push it to the repository.

abc@4816ef265cb4:~/workspace$ git add .
abc@4816ef265cb4:~/workspace$ git status
On branch main
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   .vscode/extensions.json
        new file:   .vscode/settings.json
        new file:   .vscode/tasks.json
        modified:   README.md
        new file:   aps_failure_training_set1.csv
        new file:   data_dump.py
        new file:   main.py
        new file:   requirements.txt
        new file:   steps.txt

abc@4816ef265cb4:~/workspace$ git config --global user.email arunavabiswas44@gmail.com
abc@4816ef265cb4:~/workspace$ git config --global user.name Arunava-Biswas
abc@4816ef265cb4:~/workspace$ git commit -m "This is the 1st version of code"
[main d0ef23e] This is the 1st version of code
 9 files changed, 36358 insertions(+), 1 deletion(-)
 create mode 100644 .vscode/extensions.json
 create mode 100644 .vscode/settings.json
 create mode 100644 .vscode/tasks.json
 create mode 100644 aps_failure_training_set1.csv
 create mode 100644 data_dump.py
 create mode 100644 main.py
 create mode 100644 requirements.txt
 create mode 100644 steps.txt

 - Here we need to push forcefully
 abc@4816ef265cb4:~/workspace$ git push -f origin main
Enumerating objects: 15, done.
Counting objects: 100% (15/15), done.
Delta compression using up to 64 threads
Compressing objects: 100% (12/12), done.
Writing objects: 100% (15/15), 10.23 MiB | 2.14 MiB/s, done.
Total 15 (delta 0), reused 0 (delta 0)
To https://github.com/Arunava-Biswas/aps-fault-detection.git
 + 728b447...d0ef23e main -> main (forced update)
abc@4816ef265cb4:~/workspace$ 

- Now create a file '.gitignore'
- Then again add, commit and push this to the github repo.
- The usage of ".gitignore":
    - In the source folder may be there are files which we don't want git to track, i.e. we want to prevent git from tracking those files and folders. We put those files and folders in this file.
    - Also we can prevent a very large file from getting uploaded to github by this method.





Step4
- To start a Python project the first file to be created is the 'setup.py' file.
- This 'setup.py' file is important so later we can convert our source code into library format to use it elsewhere. Example we install Pandas package to use different modules from it. So the main usage of this file is to ensure the distribution of the source code.
- Now we can create a folder here named 'sensor' inside which all the source codes will be available. So this 'sensor' folder is the Python package that we described in the setup section of the 'setup.py' file.
- Because of some specific files in the 'sensor' folder the find_packages() is able to identify as the Python package. This specific file is '__init__.py'
- So any folder contains the file '__init__.py', the 'find_packages()' will consider that folder as the Python package/library.
- Every source code requires some other libraries, so to specify the other libraries for the project we define them with 'get_requirements()'
- Here in the requirements.txt file we have used a '-e .' at the end. The usage of this is to trigger the codes in the 'setup.py' file. So this '-e .' is very important if our source code is to be used as library. So after this if we install the requirements we will see that now the sensor libray also gets installed.
- Here 'e' means editable installation and '.' is to direct to the current directory.
- Now if we install all the libraries from 'requirements.txt' we will find a new folder got created named 'sensor.egg-info'. This folder contains all the information and dependencies of the project.
- This '.egg-info' file got created due to the '-e .' This makes the code we are writing also using as a library.




Step5:
- Now we will go with the ML Project's pipeline, i.e. Data Ingestion -> Data Validation -> Data Transformation -> Model Trainer -> Model Evaluation -> Model Pusher
- Here through the jupyter notebook we came to know that the 'XGBoost' is the best model to apply here. So we will build our model on that algorithm.
- Check the flow from here https://www.tensorflow.org/tfx
- An 'Artifact' is a machine learning term that describes the output(a fully trained model, a model checkpoint or a file) created by the training process. 
- Building the pipeline:
    -  So in this project every output we get from the training pipeline will be called as 'Artifact' and every input will be termed as 'Configuration'.



Step6:
- In the Sensor folder we will create folders named 'components', 'pipeline', 'utils' and 'entity'.
- Each of these folders will have '__init__.py' files so we can convert this folder as a package.
- The 'entity' folder will hold the 'Artifact' and 'Configuration'. So in this folder the inputs and outputs will be categorized. So this will work as a structure for all the inputs and outputs for all the components and the training pipeline of the project.
- The utils folder will have all the helper functions like uploading the model to a cloud, saving the model etc.




Step7:
- Now let's start defining the inputs. Now for each component there will be some inputs. So now we need to define the inputs for each component and then the outputs for each components.
- So we will create two files in the 'entity' folder named 'config_entity.py' (for inputs) and 'artifact_entity.py' (for outputs).
- Here we will create 6 inputs (configurations) as there are 6 stages in the training pipeline. Also we need to create 6 Artifacts also in the 'artifact_entity.py' file.
- Now we need to create 6 components in the components folder.
- Also create a file for training named 'training_pipeline.py' in the pipeline folder.
- There are also two other files named 'exception' and 'logger' which are universal in any project.
- In 'logger.py' we are going to store the logging data for any user.
- In 'exception.py' we are going to put the exception messages.
- Now in 'main.py' make the necessary changes.
- Now we need to write a code so we can get the code from the mongoDB database. For this we will write it in the utils __init__.py file.
- To store the connection related codes in the 'config.py' file. Here to get the urls dynamically need to create another '.env' file and store the url there.
- Also we need to add the load_dotenv module in the root '__init__.py' file of the sensor project. So the model will read directly with the environment variables we defined in the .env file.
- Before making the changes the 'main.py' has the following code:

import pymongo

# Provide the mongodb localhost url to connect python to mongodb.
client = pymongo.MongoClient("mongodb://localhost:27017/neurolabDB")

# Database Name
dataBase = client["neurolabDB"]

# Collection  Name
collection = dataBase['Products']

# Sample data
d = {'companyName': 'iNeuron',
     'product': 'Affordable AI',
     'courseOffered': 'Machine Learning with Deployment'}

# Insert above records in the collection
rec = collection.insert_one(d)

# Lets Verify all the record at once present in the record with all the fields
all_record = collection.find()

# Printing all records present in the collection
for idx, record in enumerate(all_record):
     print(f"{idx}: {record}")





Step8:
- Now we will create the artifact directories in the config_entity file.
- So when the code get run an artifact directory will be created and inside that there will be another folder with time stamp and inside that anothe folder for data ingestion and so on.
- Also there now we can create train and test file but before that we need to give the location where we can store the mongoDB database as "feature_store_file_path". The train and test file both are staying inside 'dataset' folder.
- Now write the code of data ingestion in the component folder. This will store the data so now we can perform the model training from these datasets.