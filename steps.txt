Step1: 
- Whether git is install or not? To check just type "git" in the terminal.
- Here we are creating the project with python 3.8.10 version.
- The default mongoDB connection is: "mongodb://localhost:27017", it can be found in the 'main.py' file.





Step2:
- Create dataset inside the mongoDB database.
- To download the dataset from 'git' we use the command 'wget' in terminal.
- To download the data write in the following format and hit enter:
    wget url
- As here it is:
abc@4816ef265cb4:~/workspace$ wget https://raw.githubusercontent.com/avnyadav/sensor-fault-detection/main/aps_failure_training_set1.csv
- After doing this the dataset gets downloaded.
- Now we need to upload the data to mongoDB. To do this we will create the 'data_dump.py' file.





Step3:
- Now we will read the data from the mongoDB and do the Machine Learning project.
- But before we need to add this codes in the github using git commands.
- So go to github and create a new repo. While creating repo (aps-fault-detection) just select Add.gitignore option as 'python'.
- 1st we need to see is there any repositore already attach to this code or not? to check type the following command
    git remote -v

abc@4816ef265cb4:~/workspace$ git remote -v
origin  https://github.com/iNeuron-Pvt-Ltd/neurolab-mongodb-python (fetch)
origin  https://github.com/iNeuron-Pvt-Ltd/neurolab-mongodb-python (push)

- Now to remove the origin so we can create our own repo as origin type:
    git remote remove origin
- Again to check just type:
    git remote -v

abc@4816ef265cb4:~/workspace$ git remote remove origin
abc@4816ef265cb4:~/workspace$ git remote -v

- Now we need to add our repo as origin type:
    git remote add origin <give your url>

abc@4816ef265cb4:~/workspace$ git remote add origin https://github.com/Arunava-Biswas/aps-fault-detection.git
abc@4816ef265cb4:~/workspace$ git remote -v
origin  https://github.com/Arunava-Biswas/aps-fault-detection.git (fetch)
origin  https://github.com/Arunava-Biswas/aps-fault-detection.git (push)

- Now we need to push the changes to make the changes applicable. Here do the authentication steps to allow the repo to joined with this project. type:
    git push origin main

- Here we will get some error message, as there are changes ('.gitignore' file) in the github repo which are not shown here on this project so to make them one first we have to fetch (pull the changes from the github to the local system) the repo here then create the commit and then do the push. This is the 'merge conflict'.

- So either we need to delete the commits in the github repo or we need to delete the commits in our local system as the problem is that the commits are not matching. Here we are deleting the commits in the local repo.
- Go to hidden folder '.git'

abc@4816ef265cb4:~/workspace$ ls -a
.  ..  aps_failure_training_set1.csv  data_dump.py  .git  main.py  README.md  requirements.txt  steps.txt  .vscode

- Now go to inside the '.git' folder

abc@4816ef265cb4:~/workspace$ cd .git/
abc@4816ef265cb4:~/workspace/.git$ ls
branches  config  description  FETCH_HEAD  HEAD  hooks  index  info  logs  objects  ORIG_HEAD  packed-refs  refs

- We need to use the following command:
    git reset --soft ID
- The ID is the id of the commit where we want the pointer HEAD to move.
- With soft the codes remained.

abc@4816ef265cb4:~/workspace$ git reset --soft 6afd

- Now we can create a new commit and push it to the repository.

abc@4816ef265cb4:~/workspace$ git add .
abc@4816ef265cb4:~/workspace$ git status
On branch main
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   .vscode/extensions.json
        new file:   .vscode/settings.json
        new file:   .vscode/tasks.json
        modified:   README.md
        new file:   aps_failure_training_set1.csv
        new file:   data_dump.py
        new file:   main.py
        new file:   requirements.txt
        new file:   steps.txt

abc@4816ef265cb4:~/workspace$ git config --global user.email arunavabiswas44@gmail.com
abc@4816ef265cb4:~/workspace$ git config --global user.name Arunava-Biswas
abc@4816ef265cb4:~/workspace$ git commit -m "This is the 1st version of code"
[main d0ef23e] This is the 1st version of code
 9 files changed, 36358 insertions(+), 1 deletion(-)
 create mode 100644 .vscode/extensions.json
 create mode 100644 .vscode/settings.json
 create mode 100644 .vscode/tasks.json
 create mode 100644 aps_failure_training_set1.csv
 create mode 100644 data_dump.py
 create mode 100644 main.py
 create mode 100644 requirements.txt
 create mode 100644 steps.txt

 - Here we need to push forcefully
 abc@4816ef265cb4:~/workspace$ git push -f origin main
Enumerating objects: 15, done.
Counting objects: 100% (15/15), done.
Delta compression using up to 64 threads
Compressing objects: 100% (12/12), done.
Writing objects: 100% (15/15), 10.23 MiB | 2.14 MiB/s, done.
Total 15 (delta 0), reused 0 (delta 0)
To https://github.com/Arunava-Biswas/aps-fault-detection.git
 + 728b447...d0ef23e main -> main (forced update)
abc@4816ef265cb4:~/workspace$ 

- Now create a file '.gitignore'
- Then again add, commit and push this to the github repo.
- The usage of ".gitignore":
    - In the source folder may be there are files which we don't want git to track, i.e. we want to prevent git from tracking those files and folders. We put those files and folders in this file.
    - Also we can prevent a very large file from getting uploaded to github by this method.





Step4
- To start a Python project the first file to be created is the 'setup.py' file.
- This 'setup.py' file is important so later we can convert our source code into library format to use it elsewhere. Example we install Pandas package to use different modules from it. So the main usage of this file is to ensure the distribution of the source code.
- Now we can create a folder here named 'sensor' inside which all the source codes will be available. So this 'sensor' folder is the Python package that we described in the setup section of the 'setup.py' file.
- Because of some specific files in the 'sensor' folder the find_packages() is able to identify as the Python package. This specific file is '__init__.py'
- So any folder contains the file '__init__.py', the 'find_packages()' will consider that folder as the Python package/library.
- Every source code requires some other libraries, so to specify the other libraries for the project we define them with 'get_requirements()'
- Here in the requirements.txt file we have used a '-e .' at the end. The usage of this is to trigger the codes in the 'setup.py' file. So this '-e .' is very important if our source code is to be used as library. So after this if we install the requirements we will see that now the sensor libray also gets installed.
- Here 'e' means editable installation and '.' is to direct to the current directory.
- Now if we install all the libraries from 'requirements.txt' we will find a new folder got created named 'sensor.egg-info'. This folder contains all the information and dependencies of the project.
- This '.egg-info' file got created due to the '-e .' This makes the code we are writing also using as a library.




Step5:
- Now we will go with the ML Project's pipeline, i.e. Data Ingestion -> Data Validation -> Data Transformation -> Model Trainer -> Model Evaluation -> Model Pusher
- Here through the jupyter notebook we came to know that the 'XGBoost' is the best model to apply here. So we will build our model on that algorithm.
- Check the flow from here https://www.tensorflow.org/tfx
- An 'Artifact' is a machine learning term that describes the output(a fully trained model, a model checkpoint or a file) created by the training process. 
- Building the pipeline:
    -  So in this project every output we get from the training pipeline will be called as 'Artifact' and every input will be termed as 'Configuration'.



Step6:
- In the Sensor folder we will create folders named 'components', 'pipeline', 'utils' and 'entity'.
- Each of these folders will have '__init__.py' files so we can convert this folder as a package.
- The 'entity' folder will hold the 'Artifact' and 'Configuration'. So in this folder the inputs and outputs will be categorized. So this will work as a structure for all the inputs and outputs for all the components and the training pipeline of the project.
- The utils folder will have all the helper functions like uploading the model to a cloud, saving the model etc.




Step7:
- Now let's start defining the inputs. Now for each component there will be some inputs. So now we need to define the inputs for each component and then the outputs for each components.
- So we will create two files in the 'entity' folder named 'config_entity.py' (for inputs) and 'artifact_entity.py' (for outputs).
- Here we will create 6 inputs (configurations) as there are 6 stages in the training pipeline. Also we need to create 6 Artifacts also in the 'artifact_entity.py' file.
- Now we need to create 6 components in the components folder.
- Also create a file for training named 'training_pipeline.py' in the pipeline folder.
- There are also two other files named 'exception' and 'logger' which are universal in any project.
- In 'logger.py' we are going to store the logging data for any user.
- In 'exception.py' we are going to put the exception messages.
- Now in 'main.py' make the necessary changes.
- Now we need to write a code so we can get the code from the mongoDB database. For this we will write it in the utils __init__.py file.
- To store the connection related codes in the 'config.py' file. Here to get the urls dynamically need to create another '.env' file and store the url there.
- Also we need to add the load_dotenv module in the root '__init__.py' file of the sensor project. So the model will read directly with the environment variables we defined in the .env file.
- Before making the changes the 'main.py' has the following code:

import pymongo

# Provide the mongodb localhost url to connect python to mongodb.
client = pymongo.MongoClient("mongodb://localhost:27017/neurolabDB")

# Database Name
dataBase = client["neurolabDB"]

# Collection  Name
collection = dataBase['Products']

# Sample data
d = {'companyName': 'iNeuron',
     'product': 'Affordable AI',
     'courseOffered': 'Machine Learning with Deployment'}

# Insert above records in the collection
rec = collection.insert_one(d)

# Lets Verify all the record at once present in the record with all the fields
all_record = collection.find()

# Printing all records present in the collection
for idx, record in enumerate(all_record):
     print(f"{idx}: {record}")





Step8:
- Now we will create the artifact directories in the config_entity file.
- So when the code get run an artifact directory will be created and inside that there will be another folder with time stamp and inside that anothe folder for data ingestion and so on.
- Also there now we can create train and test file but before that we need to give the location where we can store the mongoDB database as "feature_store_file_path". The train and test file both are staying inside 'dataset' folder.
- Now write the code of data ingestion in the component folder. This will store the data so now we can perform the model training from these datasets.
- Also write the code for class for data ingestion in the 'config_entity.py' and 'artifact_entity.py' files.

- Remember to load the the dataset into artifact folder eachtime when we start the vscode we need to first connect with mongoDB using clicking the connect mongoDB option in the mongoDB icon, then dump the data into the database using "python data_dump.py" then we need to run the main.py file using "python main.py". Also we need to install the pandas as it is not there in the requirements.txt file.
- Or we can add the "pandas==1.5.2" in the requirements.txt file.
- Also now we can rename the '__init__.py' file inside the utils folder as utils.py and keep it in the main workspace, i.e. no need to have any utils folder.




Step9:
- Let's start the validation of the data. To validate the data we need to match the number of features, their data types and their names.
- Here we will use the base .csv file (i.e. aps_failure_training_set1) to validate the train and test file we created. Here we will validate the data with respect to number of features, data types and the column names.
- To validate the data we will use the 'ks_2samp' from the 'scipy' library.
- Here we will compare between the train and test file with the base file to check whether the columns belong to same distribution or not. If the distribution is not matched we call that condition as 'DRIFT'.
- Now we will write code in the class related to transformation in the 'config_entity.py' and 'artifact_entity.py' files also we need to write code in the data_validation.py file in the component folder.




Step10:
- Doing same steps as above for the transformation of data. It is same as Data Preprocessing.
- Here we will use the SMOTE as the data is imbalanced. Here we will increase the record of the minority class to make them equal.
- Then we will create a pipeline with SimpleImputer with strategy as 'constant' as showed in the jupyter notebook during EDA. So all the missing values in a column will be replaced by the 0 value or we can also use the 'mean'.
- After that we will use the RobustScaler() for standardizing/scaling the data, as the data has some outliers.
- Here we need to save 3 things:
        - Transformed train dataset
        - Transformed test dataset
        - Transformation object (to use later for the prediction pipeline as there also we need to do the same transformation as we are doing here)



Step11:
- Now we will do the model training.
- Here for classification we will look for 'F1' score.




Step12:
- Now we will do the model evaluation.
- Here we will compare between the trained model with the latest model we have, so we can get the best model for production.
- This is the continuous training, i.e. everytime it trains and find a new training model it compares that with the current model it is using and check for the best model to use for future.
- As here we have the trained model in the 'artifact/user_folder/model_trainer/model/model.pkl' location.
- So we need to write a code where we can maintain all the 3 files (model.pkl, transformer.pkl, target_encoder.pkl) together, so we create a new file named Predictor.py.
- This will create a new folder named 'saved_mdel' and there it will keep all the latest models to do the comparisons.




Step13:
- Now creating the model evaluation.
- The ModelResolver class in predictor.py file helps to save the model at a particular location and again to load that model from that location dynamically.
- Here we need to compare between the train model and the saved model.
- Here can be 2 cases took place:

    Case1: 
        - We don't have any model to compare in model evaluation.
        - This will happen when we run the model prediction for the first time.

    Case2:
        - We have a model.
        - Here we need to make a comparison between the previous model and the present model.



Step14:
- Now doing the model pusher.
- So here we used the 'saved_models' folder containing the 3 .pkl files in a folder named as integer.
- Also we need to create the prediction API.
- This API will always load objects (transformer and target encoder) from the latest folder. With these it will transform any new input data and give predictions.
- The task for model_pusher.py is to save the models inside the 'saved_models' folder.
- At end we can sync this 'saved-model' with the AWS S3 bucket.

- Right now in the ModelResolver class in predictor.py there is a bug. As of now we have created functions that will provide the locations of the model, transformer and target_encoder. But here each functions is dependent on the 'get_latest_save_dir_path()' which always increase the previous directory and returns the latest directory. So if we try to save the models, then instead of storing all 3 of them in a single folder it will save them in 3 different folders organised serially.
- To overcome this problem we created 3 different paths in the 'model_pusher.py'file named 'transformer_path', 'model_path' and 'target_encoder_path'. After that if we save the 3 files then it will not create the previous problem. So here first we are getting the locations and then doing the save part.
- Here when we run the code we get an error :
Error occurred python script name [/config/workspace/sensor/components/model_evaluation.py] line number [105] error message [Current trained model is not better than previous model]
- This is due to the usage of SMOTE where everytime it generates new data and as a result the accuracy of the new model is less than the previous one. As a result we don't get a new model in the saved_model folder and also there is no model_pusher folder in the artifact folder.
- To overcome this we can use the randomstate as 42 in the data ingestion during train_test split, and also in data transformation in SMOTE we can pass the same random state. 


- Now we are going to cut the entire code of the main.py file and save it in the 'training_pipeline.py' file of the pipeline folder.

- Now we can import this training_pipeline in the main and run it.
- We are also going to create another file named 'bath_prediction.py' in the pipeline folder. 
- Here we will create a prediction directory, then reading the input files, transformers, models and then we do predictions and the results are stored as files in this folder.







- Now for the deployment:
- First develop the program code.
- Store the code in github.
- Now to do continuous training of the code like the (training_pipeline or batch_prediction) we have to use some sort of scheduler. This is similar to the Continuous Learning.
- Here we need to schedule the training_pipeline so it can run continuously.
- Here we will use "Airflow" library to schedule our job. This will works as a scheduler for the training_pipeline and batch_prediction.
- In case we have a python source code to deploy we need to follow the following steps:
    - Create Python environment.
    - Install the dependencies.
    - Run the Script.

- But these steps are dependent on the machine we are using, so we need to make the deployment independent of these machines, for this we have DOCKER.
- By using DOCKER we can package the code and move from one place to another.
- In Docker we need to follow the following steps:
    - Choose a Base machine (independent of local machine).
    - Copy code from local system to Docker machine.
    - Install dependencies.
    - Run the script.
    - Create the Docker image.
    - Then run the image.


- Deployment steps:
    1. Create a DOCKER image.
    2. Run the DOCKER image.


- Github:
    - Clone Source code.
    - Build DOCKER image.
    - Store the image (Elastic Container Registry in AWS) [ECR: Docker image storage]
    
- Now we will run the image in EC2 (Elastic Compute Cloud). For this we need to first download the image and then run the image.
- And the deployment is done.

- Now mongoDB is not accessible in EC2. To do this we need to use the mongoDB Atlas.

Step1: Get MongoDB Atlas url.
Step2: Connect with the url (to check whether the connection is happened or not).
Step3: Change the url in the .env file.
Step4: Run the data_dump.py file to insert the data inside the Atlas. To do this we can import the MongoClient in data_dump.py file.
Step5: Create a Docker file. To do this just name the new file as "Dockerfile" and that file will become a Docker File.
- Follow the steps in the Dockerfile:
        # base image
        FROM python:3.8
        # make ourselves as root user
        USER root
        # create a folder as '/app' to copy the code
        RUN mkdir /app
        # copy source code from this directory to the '/app' folder
        COPY . /app/
        # setting the working directory to the '/app' location
        WORKDIR /app/
        # now installing the dependencies
        RUN pip3 install -r requirements.txt

        # setting environment variables for Air flow
        # Setting the home variable for airflow i.e. where the airflow code will be written
        ENV AIRFLOW_HOME="/app/airflow"
        # to load how much time it should wait
        ENV AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=1000
        ENV AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True

        # Commands related to airflow
        # initializing the airflow database
        RUN airflow db init 
        # creating an user to login to the airflow portal
        # syntax is:
        # RUN airflow users create  -e <email> -f <fname> -l <lname> -p <password> -r <role> -u <username>
        RUN airflow users create  -e avnish@ineuron.ai -f Avnish -l Yadav -p admin -r Admin  -u admin

        # Create a file named start.sh
        # This is a script containing the shell commands

        # now give permision to this script
        RUN chmod 777 start.sh
        # installing the awscli so we can save the folders like 'artifact', 'saved_models' inside the S3 bucket
        RUN apt update -y && apt install awscli -y
        # entry point to state where the cell is located
        ENTRYPOINT [ "/bin/sh" ]
        # to run the 'start.sh' script
        CMD ["start.sh"]


Step6: Create another file named 'docker-compose.yaml'. This file will be used to ensure all the names we used in the project remain same.
Step7: Create another file named '.dockerignore', so when we build the docker image we don't need all the folders of the source code. It is similar to '.gitignore'.
Step8: Now create another folder named 'airflow', in which we need another folder named 'dags'. Inside this we will create the 'training_pipeline.py' and 'batch_prediction.py' files. The codes inside the files can be found in airflow.
- 'DAG' is Directed Acyclic Graph which is created in airflow.



- Create another folder named '.github' and a 'workflows' folder inside that. In this folder we will write all the codes of github actions (the workflow in github i.e. the orders of actions) in a 'main.yaml' file.
- Here we will create codes for github actions, so the code will run automatically.

- Now we will need to set all the values wherever we find 'secrets' like:
     AWS_ACCESS_KEY_ID=<Access key ID>
     AWS_SECRET_ACCESS_KEY=<Secret access key>
     AWS_REGION=ap-south-1
     AWS_ECR_LOGIN_URI=<ECR url (the 1st part upto '/')>
     ECR_REPOSITORY_NAME=<ECR repo name (the part after the '/')>
     BUCKET_NAME=<bucket name from S3>
     MONGO_DB_URL=<mongodb atlas url>

- We need to set these many variables information.
- Login to AWS
- Now we need to create the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:
    - So search for IAM (Identity and Access Management) service in the search box and select the option IAM.
    - This IAM is the service using which we can access and manage the AWS resources in AWS cloud.
    - In the IAM dashboard we need to create an user by clicking 'Add user' and create an user, also check the box 'Access key - Programmatic access' in the 'Select AWS credential type'.
    - Then click on 'Next: Permissions'.
    - Now we need to attach the policy, for this select the option 'Attach existing policy directly', here we need to check box against the option 'AdministratorAccess'.
    - Click on 'Next' and then click 'Create user'.
    - Now it will create an user and we can download a .csv file containing the information we want.
    - So in this file we will get the Access key ID and Secret access key.
    - Now add these information in a notepad along with the variable names.
    - Remember these details not to be shared with anyone. 

    - Now again search for ECR (Elastic Container Registry)
    - This is where the Docker image is going to be stored, so it can be used in the EC2 instance.
    - Check the location in the right top side and choose 'Asia Pacific (Mumbai)' region.
    - Here we need to create a repository to store the Docker image like we created a repository in github to store the code.
    - Click Create Repository -> give a name -> Create repository.
    - Now we will get the repository name and an url. Copy this url and put in the required boxes.

    - Now we will create a bucket to get the bucket name.
    - Now search S3 and choose Create Bucket option, choose Mumbai location, give a Bucket Name.
    - Remember this Bucket Name must be universally unique (not to be used by anyone anywhere like Aadhar number) -> Create Bucket.


- Now once all these information we get then we need to create an EC2 machine and then we can start the deployment.
    - To create an EC2 machine search for EC2.
    - Make sure that the region is set as Mumbai.
    - Click instances.
    - Click Launch instances.
    - Give a name to the machine.
    - Now choose a machine (here we will choose an ubuntu machine)
    - In Key pair option select Create new Key pair -> Key pair name (give any name) -> Create key.
    - This will download a 'ec2.pem' key.
    - Now we can increase the HDD space in Configure storage section. Set it to 30 GB.
    - Click 'Launch Instances'.


- Now search the machine by clicking on instances and wait until the status becomes '2/2 checks passed'.
- Now click on the check button next to the machine name and go to the Security tab.
- Now in there Security -> Security Groups (click on the link inside here) -> Inbound Rules
- Here this machine can only accept SSH requests but we will need http port, so we need to configure http for this machine.
- Click 'Edit inbound rules' -> 'Add rule' -> Choose 'All traffic'(in Type) -> 'Anywhere-IPV4'(in Source) -> 'Save rules'.
- Now select the new object we created.
- Now again search the machine by clicking on instances.
- Now click on the check button next to the machine name and click the 'Connect' button at the top.
- In the new page 'Connect to instance' click on the 'Connect'.
- It will take some time and then the machine will open.

- Now install Docker at first here, commands are

curl -fsSL https://get.docker.com -o get-docker.sh (to download the script get-docker.sh)
ls (to check whether the script got downloaded or not)
sudo sh get-docker.sh (to execute the downloaded file, after this docker will be installed in this machine)
sudo usermod -aG docker ubuntu 
newgrp docker 

- As a result docker will installed to check type docker --version.

- Open the github repository and open the settings(gear sign)
- Go to the 'Actions' option select 'Runners' option there.
- Now configure this Runners page.
- Click 'New self-hosted runner'
- Here we have to choose 'Linux'
- Now run the commands given here in the Download and Configure sections in the EC2 machine one by one.
- In Configure commands:
    - Press Enter for first time when asking the name of the runner Group
    - Type 'self-hosted' when again asked for name of the runner Group
    - Press Enter (enter any additional label)
    - Press Enter again (enter name of work folder)
    - Now ./run.sh

- it will show 'Connected to GitHub'.
- Now go to github again and open runners and there will be a self-hosted as idle status.
- Now go to the secrests option in the github settings and there go to Actions secrets page by selecting actions.
- Now click 'New Repository Secret' and there in name put the key like "AWS_ACCESS_KEY_ID" and in the secrets set the values one by one for all the secrets we have collected.

- Now for the final deployment:
- go to the Actions and see whether it is working or not.
- Add in reuirement.txt 'apache-airflow'
- If everything go alright we will find the Docker image in the ECR. 